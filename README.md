# ðŸš† Delay Prediction in Dutch Train Services

A machine learning project aimed at predicting significant train delays in the Netherlands using real-world passenger train service data from 2024.

The dataset comes from [Rijden de Treinen](https://www.rijdendetreinen.nl/en/open-data/train-archive) and contains Dutch passenger train service records for the full year of **2024**.

---

## Dataset Description

Each row in the dataset represents **a stop of a train service at a station**. A service contains multiple stops, starting with an initial departure and ending with a final arrival.

**Key columns:**

| Column | Description |
|---|---|
| `Service:RDT-ID` | Unique identifier for a service on a given date |
| `Service:Date` | Scheduled service date |
| `Service:Type` | Train type (e.g., Intercity, Sprinter) |
| `Service:Company` | Operating company |
| `Service:Maximum delay` | Maximum delay recorded across all stops (in minutes) |
| `Stop:Station name` | Name of the station |
| `Stop:Arrival time` | Planned arrival time |
| `Stop:Departure time` | Planned departure time |
| `Stop:Arrival delay` | Arrival delay in minutes |
| `Stop:Departure delay` | Departure delay in minutes |
| `Stop:Arrival cancelled` | Whether the arrival was cancelled |
| `Stop:Departure cancelled` | Whether the departure was cancelled |

**Dataset size:** ~21.8 million rows Ã— 20 columns (before cleaning)

---

## Project Structure

```
â”œâ”€â”€ EDA.ipynb                    # Exploratory Data Analysis & Data Cleaning
â”œâ”€â”€ logistic_regression.ipynb    # Model 1: Logistic Regression (baseline)
â”œâ”€â”€ df_clean.csv                 # Cleaned dataset (generated by EDA notebook)
â””â”€â”€ README.md
```

---

## Notebook 1 â€” Exploratory Data Analysis (`EDA.ipynb`)

### Data Cleaning

The raw dataset required careful structural validation before any modeling could take place:

- **Dropped irrelevant columns:** `Stop:Station code`, `Stop:Planned platform`, `Stop:Actual platform` (high missingness, not central to delay prediction).
- **Handled structural nulls:** The first stop of each service has no arrival time, and the last stop has no departure time â€” these are expected by design and preserved.
- **Filtered intermediate stops:** Only middle stops with complete delay-related information were kept. Corrupted intermediate rows were removed.
- **Type conversions:** Time columns converted to `datetime`, cancellation columns to `bool`, delay columns to `numeric`.

The cleaned dataset was saved as `df_clean.csv`.

### Data Visualisation

Key insights explored during EDA:

- **Delay distribution:** Heavily right-skewed â€” the vast majority of services run on time or with minimal delays, while a small proportion experience severe delays (> 15 min).
- **Temporal patterns:** Delay rates vary by month, weekday, and hour of departure, revealing seasonal trends and rush hour effects.
- **Class imbalance:** Approximately **9% of services** experience a significant delay (> 5 min) vs. **91% on time** â€” a major challenge for modeling.
- **Service-level aggregation:** Since `Service:Maximum delay` is meaningful only at the service level, stop-level data was aggregated per `Service:RDT-ID` for modeling.

---

## Notebook 2 â€” Logistic Regression (`logistic_regression.ipynb`)

### Objective

The goal is to predict whether a train service will experience a **significant delay**, defined as a maximum delay strictly greater than **5 minutes**.

The target variable is binary:
- `1` â†’ the service has a maximum delay **> 5 minutes**
- `0` â†’ the service is on time or has a minor delay (â‰¤ 5 minutes)
  
### Feature Engineering

Time-based features were extracted to capture:
- Rush hour patterns
- Weekend vs. weekday behaviour
- Seasonal variation

**Features used in the model:**

| Feature | Type | Description |
|---|---|---|
| `Service:Type` | Categorical | Train type |
| `Service:Company` | Categorical | Operating company |
| `month` | Numerical | Month of service (1â€“12) |
| `weekday` | Categorical | Day of the week |
| `dep_hour` | Numerical | Departure hour |
| `prop_cancelled_stops` | Numerical | Proportion of cancelled stops per service |
| `mean_arrival_delay` | Numerical | Mean arrival delay across stops |
| `mean_departure_delay` | Numerical | Mean departure delay across stops |

### Modeling Pipeline

A **scikit-learn Pipeline** was used, combining:
1. `StandardScaler` â€” normalizes numerical features to zero mean and unit variance.
2. `LogisticRegression` with:
   - `class_weight='balanced'` â€” corrects for class imbalance
   - `max_iter=1000` â€” ensures convergence on large datasets
   - `solver='lbfgs'` â€” efficient for this scale

An **80/20 stratified train/test split** was used to preserve class proportions.

### Evaluation Metrics

Given the class imbalance, the following metrics were used:
- **F1-score** (weighted and per class)
- **AUC-ROC** â€” overall class separability
- **PR-AUC** (Average Precision) â€” more informative than ROC for imbalanced data
- **Confusion Matrix**

### Results

| Metric | Score |
|---|---|
| AUC-ROC | 0.768 |

The AUC-ROC of 0.768 indicates a fair ability to distinguish between classes. However, the heavy class imbalance (~9% vs. 91%) limits the model's recall for the minority class, which is the primary target of this prediction task.

### Feature Importance

Logistic Regression coefficients were analyzed to identify the most influential features on delay prediction:
- **Positive coefficients** â†’ increase probability of significant delay
- **Negative coefficients** â†’ decrease probability of significant delay

This interpretability is one of the main advantages of Logistic Regression as a baseline.

---

## Class Imbalance Strategy

The dataset is heavily imbalanced (â‰ˆ 9% delayed vs. 91% on-time). Strategies applied and considered:

- `class_weight='balanced'` in Logistic Regression
- Evaluation with PR-AUC instead of relying solely on accuracy
- Future iterations will explore resampling techniques (SMOTE, undersampling)

---

## Next Steps

The Logistic Regression model serves as a **baseline**. The next phase of this project will proceed with two more powerful models:

### Model 2 â€” Gradient Boosting: XGBoost & LightGBM
Tree-based ensemble models that natively handle class imbalance, non-linear relationships, and high-dimensional tabular data. These models are expected to significantly outperform logistic regression in terms of F1-score and PR-AUC.

### Model 3 â€” LSTM (Long Short-Term Memory)
A recurrent neural network architecture that can capture **sequential dependencies** between stops within a service. Since a train's delay at one stop is likely influenced by delays at previous stops, LSTM is a natural fit for this temporal, sequence-based structure of the data.
